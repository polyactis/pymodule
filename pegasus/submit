#!/bin/bash

set -e

TOPDIR=`pwd`

storageSiteNameDefault="local"
submitOptionDefault="--submit"
scratchTypeDefault="1"
cleanupClustersSizeDefault=15

#2013.04.24
runningWorkflowLogFnameDefault=runningWorkflows.txt
failedWorkflowLogFnameDefault=failedWorkflows.txt

# figure out where Pegasus is installed
export PEGASUS_HOME=`which pegasus-plan | sed 's/\/bin\/*pegasus-plan//'`
if [ "x$PEGASUS_HOME" = "x" ]; then
	echo "Unable to determine location of your Pegasus install"
	echo "Please make sure pegasus-plan is in your path"
	exit 1
fi 
echo "pegasus home is " $PEGASUS_HOME

if [ "x$GLOBUS_LOCATION" = "x" ]; then
	echo "Please set GLOBUS_LOCATION to the location of your Pegasus install"
	exit 1
fi 
 
echo GLOBUS_LOCATION: $GLOBUS_LOCATION

# 2011-8-28 same as the submitted user's home directory
# it's a must to export HOME in condor environment because HOME is not set by default.
HOME_DIR=$HOME

PEGASUS_PYTHON_LIB_DIR=`$PEGASUS_HOME/bin/pegasus-config --python`
VCF_PERL5LIB=script/vcftools/perl

UCLA_CLUSTER_HOSTNAME="grid4.hoffman2.idre.ucla.edu"
UCLA_CLUSTER_SCHEDULER="sge"
UCLA_CLUSTER_HOME="/u/home/eeskin/polyacti"
UCLA_CLUSTER_PEGASUS_HOME="$UCLA_CLUSTER_HOME/bin/pegasus"
UCLA_CLUSTER_GLOBUS_LOCATION="/home/globus/gt5.0.4"
UCLA_CLUSTER_WORK_DIR=$UCLA_CLUSTER_HOME"/pg_work"

UCLA_CLUSTER_SCRATCH_Type1="$UCLA_CLUSTER_HOME/NetworkData/scratch/"
UCLA_CLUSTER_SCRATCH_Type2="/u/scratch/p/polyacti/pegasus"
# 2013.03.21 the old scratch
# 2013.03.28 replaced by the new scratch completely
UCLA_CLUSTER_SCRATCH_Type3="/u/scratch/p/polyacti/pegasus"
# 2013.3.6 on 2013.3.4, hoffman2 upgraded to the new /u/scratch2 system. this will phase out eventually
#UCLA_CLUSTER_SCRATCH_Type1_old="/u/scratch/p/polyacti/pegasus"

UCLA_CLUSTER_PEGASUS_CONFIG_PATH=$UCLA_CLUSTER_PEGASUS_HOME/bin/pegasus-config
if test -f $UCLA_CLUSTER_PEGASUS_CONFIG_PATH; then
	#if no test file existence, error will happen
	UCLA_CLUSTER_PEGASUS_PYTHON_LIB_DIR=`$UCLA_CLUSTER_PEGASUS_CONFIG_PATH --python`
else
	UCLA_CLUSTER_PEGASUS_PYTHON_LIB_DIR=$UCLA_CLUSTER_PEGASUS_HOME/lib/pegasus/python
fi

UCLA_CLUSTER_PYTHON_DIR="/u/local/python/2.6/"
UCLA_CLUSTER_R_DIR="/u/local/apps/R/R-2.12.2-gnu/"
UCLA_CLUSTER_HDF5_DIR="/u/local/intel/11.1/libs/hdf5/1.8.8-shared/"	#2013.1.10 not necessary

freeSpace="50000G"

USC_CLUSTER_HOSTNAME="hpc-login2.usc.edu"
USC_CLUSTER_SCHEDULER="pbs"
USC_CLUSTER_HOME="/home/cmb-03/mn/yuhuang"
USC_CLUSTER_WORK_DIR=$USC_CLUSTER_HOME"/pg_work"
USC_CLUSTER_PEGASUS_HOME=$USC_CLUSTER_HOME"/bin/pegasus"
USC_CLUSTER_GLOBUS_LOCATION="/usr/usc/globus/default/"
#USC_CLUSTER_GLOBUS_LOCATION="/usr/local/globus/default"
USC_CLUSTER_PEGASUS_CONFIG_PATH=$USC_CLUSTER_PEGASUS_HOME/bin/pegasus-config
if test -f $USC_CLUSTER_PEGASUS_CONFIG_PATH; then
	#if no test file existence, error will happen
	USC_CLUSTER_PEGASUS_PYTHON_LIB_DIR=`$USC_CLUSTER_PEGASUS_CONFIG_PATH --python`
else
	USC_CLUSTER_PEGASUS_PYTHON_LIB_DIR=$USC_CLUSTER_PEGASUS_HOME/lib/pegasus/python
fi


if test $# -lt 2 ; then
	echo "Usage:"
	echo "  $0 dagFile computingSiteName [keepIntermediateFiles] [cleanupClustersSize] [scratchType] [submitOption] [storageSiteName] [finalOutputDir] [submitFolderName]"
	echo ""
	echo "Note:"
	echo "	#. computingSiteName is the site on which the computing jobs run. could be local, ycondor, hcondor, hoffman2, uschpc. Setup in the dagFile should match this."
	echo ""
	echo "	#. keepIntermediateFiles (setting it to 1 means no cleanup, otherwise or default is cleanup.). It changes the default submit option ($submitOptionDefault) to --submit --nocleanup."
	echo "	#. cleanupClustersSize controls how many jobs get clustered into one on each level. Default is $cleanupClustersSizeDefault ."
	echo "	#. scratchType is the type of scratch file system to use."
	echo "		Only valid for hcondor site. "
	echo "			1: scratch FS is  $UCLA_CLUSTER_SCRATCH_Type1."
	echo "			2: scratch FS is  $UCLA_CLUSTER_SCRATCH_Type2."
	echo
	echo "	#. submitOption are options passed to pegasus-plan. Default is $submitOptionDefault. "
	echo "		'--submit' means pegasus will plan & submit the workflow."
	echo "		'--submit --nocleanup' means pegasus will not add cleanup jobs = all intermediate files will be kept."
	echo "		If you set it to empty string, '', only planning will be done but no submission."
	echo "		This option overwrites the keepIntermediateFiles option, which modifies the default submitOption."
	echo "	#. storageSiteName is the site to which final output is staged to. default is $storageSiteNameDefault."
	echo "	#. finalOutputDir is the directory which would contain the files requested to be transferred out by the end of the workflow. If it doesn't exist, pegasus would create one. Default is dagFile (without first folder if there is one) + year+date+time"
	echo "	#. submitFolderName is the submit folder which contains all job description files, job stdout/stderr output, logs, etc.It is optional. If not given, value of finalOutputDir is taken as submitFolderName."
	echo "	#. finalOutputDir and submitFolderName could be same. But they should be different from previous workflows."
	echo ""
	echo "Examples:"
	echo "	#plan & submit and do not keep intermediate files, cleanup clustering=30"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml ycondor"
	echo "	$0 dags/TrioInconsistency/TrioInconsistency15DistantVRC.xml ycondor 0 30"
	echo
	echo "	#plan & submit, keep intermediate files, cleanupClustersSize=10 doesn't matter, run on scratch type 2 ($UCLA_CLUSTER_SCRATCH_Type2) on hcondor"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml hcondor 1 10 2"
	echo
	echo "	#only planning, no running (keepIntermediateFiles & cleanupClustersSize & scratchType do not matter) by assigning empty spaces to submitOption"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml ycondor 0 20 1 \"  \" "
	echo
	echo "	#run the workflow while keeping intermediate files. good for testing in which you often need to modify .dag.rescue log to backtrack finished jobs"
	echo "	$0 dags/TrioInconsistency/TrioInconsistency15DistantVRC.xml hcondor 1 20 1 \"--submit\" local TrioInconsistency/TrioInconsistency15DistantVRC_20110929T1726 TrioInconsistency/TrioInconsistency15DistantVRC_20110929T1726 "
	exit 1
fi

dagFile=$1
computingSiteName=$2
keepIntermediateFiles=$3
cleanupClustersSize=$4
shift
scratchType=$4
submitOption=$5
storageSiteName=$6
finalOutputDir=$7
submitFolderName=$8

#2013.2.12 no cleanup only when keepIntermediateFiles = 1
if test "$keepIntermediateFiles" = "1"; then
	submitOptionDefault="--submit --nocleanup"
fi

#2013.03.26
if test -z "$cleanupClustersSize"
then
	cleanupClustersSize=$cleanupClustersSizeDefault
fi

echo cleanupClustersSize is $cleanupClustersSize.

#2013.2.12 
if test "$scratchType" = "2"; then
	UCLA_CLUSTER_SCRATCH=$UCLA_CLUSTER_SCRATCH_Type2
elif test "$scratchType" = "3"; then
	UCLA_CLUSTER_SCRATCH=$UCLA_CLUSTER_SCRATCH_Type3
else
	UCLA_CLUSTER_SCRATCH=$UCLA_CLUSTER_SCRATCH_Type1
fi

echo "Default submitOption is changed to $submitOptionDefault."

if test -z "$submitOption"
then
	submitOption=$submitOptionDefault
fi


if [ -z $storageSiteName ]; then
	storageSiteName=$storageSiteNameDefault
fi


if [ -z $finalOutputDir ]; then
	t=`python -c "import time; print time.asctime().split()[3].replace(':', '')"`
	month=`python -c "import time; print time.asctime().split()[1]"`
	day=`python -c "import time; print time.asctime().split()[2]"`
	year=`python -c "import time; print time.asctime().split()[-1]"`
	finalOutputDir=`python -c "import sys, os; pathLs= os.path.splitext(sys.argv[1])[0].split('/'); n=len(pathLs); print '/'.join(pathLs[-(n-1):])" $dagFile`.$year.$month.$day\T$t;
	echo Final output will be in $finalOutputDir
fi

if test -z "$submitFolderName"
then
	submitFolderName=$finalOutputDir
fi

#UCLA_CLUSTER_LOCAL_SCRATCH="/work/polyacti/pegasus"
#if you choose hcondor as storage site, final output is staged out to local scratch!! make sure it's in shared storage.
UCLA_CLUSTER_LOCAL_SCRATCH=$UCLA_CLUSTER_SCRATCH

runningWorkflowLogFname=$runningWorkflowLogFnameDefault
failedWorkflowLogFname=$failedWorkflowLogFnameDefault

echo "Submitting to $computingSiteName for computing."
echo "runningWorkflowLogFname is $runningWorkflowLogFname."
echo "failedWorkflowLogFname is $failedWorkflowLogFname."
echo "UCLA_CLUSTER_SCRATCH is $UCLA_CLUSTER_SCRATCH."
echo "UCLA_CLUSTER_LOCAL_SCRATCH is $UCLA_CLUSTER_LOCAL_SCRATCH."
echo "storageSiteName is $storageSiteName."
echo "Final workflow submit option is $submitOption."



# 2012.7.31 the two lines below are added to any condor cluster that do not use shared file system or filesystem that is not good at handling numerous small files in one folder
#		<profile namespace="condor" key="should_transfer_files">YES</profile>
#		<profile namespace="condor" key="when_to_transfer_output">ON_EXIT_OR_EVICT</profile>

# create the site catalog
cat >sites.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0">
	<site  handle="local" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<head-fs>
			<scratch>
				<local>
					<!-- this is used by condor local universe executables (i.e. pegasus-cleanup/transfer if you setup that way) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
					<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
					<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</scratch>
			<storage>
				<!-- this is where the final output will be staged (when the storageSiteName is "local", default) -->
				<local>
					<!-- this is used when pegasus-cleanup/transfer are set in condor local universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<!-- this is used when pegasus-cleanup/transfer are set in condor vanilla universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="env" key="PEGASUS_HOME" >$PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$GLOBUS_LOCATION</profile>
		<profile namespace="env" key="HOME">$HOME</profile>
		<profile namespace="env" key="PERL5LIB">$HOME_DIR/$VCF_PERL5LIB</profile>
		<profile namespace="env" key="PATH" >$HOME_DIR/bin:$PATH</profile>
		<profile namespace="env" key="PYTHONPATH">$PYTHONPATH:$PEGASUS_PYTHON_LIB_DIR</profile>
	</site>
	<site  handle="ycondor" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
				<!-- this is where the computing output will be at for ycondor site -->
				<local>
					<!-- this is used by condor local universe executables (i.e. pegasus-cleanup/transfer if you setup that way) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
					<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<!-- this is used by condor vanilla universe executables (most executables should be) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
					<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</scratch>
			<storage>
				<!-- this is where the final output will be staged when the storageSiteName is "ycondor", otherwise never used. -->
				<local>
					<!-- this is used when pegasus-cleanup/transfer are set in condor local universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<!-- this is used when pegasus-cleanup/transfer are set in condor vanilla universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="pegasus" key="style" >condor</profile>
		<profile namespace="condor" key="universe" >vanilla</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$PEGASUS_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$HOME_DIR/$VCF_PERL5LIB</profile>
		<profile namespace="env" key="HOME" >$HOME_DIR</profile>
		<profile namespace="env" key="PATH" >$HOME_DIR/bin:$PATH</profile>
		<profile namespace="env" key="PYTHONPATH">$PYTHONPATH:$PEGASUS_PYTHON_LIB_DIR</profile>
	</site>
	<site  handle="hcondor" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
				<!-- this is where the computing output will be at for hcondor site -->
				<local>
					<!-- this is used by condor local universe executables (some cleanup/stage-in/out jobs if you setup that way) -->
					<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH"/>
					<internal-mount-point mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<!-- this is used by condor vanilla universe executables (most executables should be) -->
					<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_SCRATCH"/>
					<internal-mount-point mount-point="$UCLA_CLUSTER_SCRATCH" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</scratch>
			<storage>
				<!-- this is where the final output will be staged when the storageSiteName is "hcondor", otherwise never used. -->
				<local>
					<!-- this is used when pegasus-cleanup/transfer are set in condor local universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</local>
				<shared>
					<!-- this is used when pegasus-cleanup/transfer are set in condor vanilla universe) -->
					<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
					<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="pegasus" key="style" >condor</profile>
		<profile namespace="condor" key="universe" >vanilla</profile>
		<profile namespace="condor" key="should_transfer_files">YES</profile>
		<profile namespace="condor" key="when_to_transfer_output">ON_EXIT_OR_EVICT</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$UCLA_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$UCLA_CLUSTER_HOME/$VCF_PERL5LIB</profile>
		<profile namespace="env" key="HOME" >$UCLA_CLUSTER_HOME</profile>
		<profile namespace="env" key="PATH" >$UCLA_CLUSTER_HOME/bin:$PATH:$UCLA_CLUSTER_HDF5_DIR/bin</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$UCLA_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="env" key="LD_LIBRARY_PATH" >$UCLA_CLUSTER_HOME/lib:$UCLA_CLUSTER_R_DIR/lib64/R/lib:$UCLA_CLUSTER_PYTHON_DIR/lib:/u/local/intel/11.1/openmpi/1.4.2/lib:/u/local/compilers/intel/11.1/073/mkl/lib/em64t:/u/local/compilers/intel/11.1/073/lib/intel64:$UCLA_CLUSTER_HDF5_DIR/lib</profile>
		
		<profile namespace="env" key="OMPI_MCA_mpi_leave_pinned">1</profile>
		<profile namespace="env" key="OMPI_MCA_mpi_warn_on_fork">0</profile>
		<profile namespace="env" key="R_BIN">$UCLA_CLUSTER_R_DIR/bin</profile>
		<profile namespace="env" key="R_DIR">$UCLA_CLUSTER_R_DIR</profile>
		<profile namespace="env" key="R_HOME">$UCLA_CLUSTER_R_DIR/lib64/R</profile>
		<profile namespace="env" key="PYTHON_DIR">$UCLA_CLUSTER_PYTHON_DIR</profile>
		<profile namespace="env" key="PYTHON_INC">$UCLA_CLUSTER_PYTHON_DIR/include/python2.6</profile>
		<profile namespace="env" key="PYTHON_LIB">$UCLA_CLUSTER_PYTHON_DIR/lib</profile>
		<profile namespace="env" key="PYTHONPATH">$UCLA_CLUSTER_HOME/lib/python:$UCLA_CLUSTER_PYTHON_DIR/lib64/python2.6/site-packages:$UCLA_CLUSTER_PYTHON_DIR/lib/python2.6/site-packages:$UCLA_CLUSTER_PEGASUS_PYTHON_LIB_DIR:$UCLA_CLUSTER_PYTHON_DIR/lib64/python2.6-other</profile>
	</site>
	<site  handle="hoffman2" arch="x86_64" os="LINUX">
		<grid  type="gt5" contact="$UCLA_CLUSTER_HOSTNAME/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt5" contact="$UCLA_CLUSTER_HOSTNAME/jobmanager-$UCLA_CLUSTER_SCHEDULER" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
				<shared>
					<file-server protocol="gsiftp" url="gsiftp://$UCLA_CLUSTER_HOSTNAME/" mount-point="$UCLA_CLUSTER_WORK_DIR"/>
					<internal-mount-point mount-point="$UCLA_CLUSTER_WORK_DIR" />
				</shared>
			</scratch>
			<storage>
				<shared>
					<file-server protocol="gsiftp" url="gsiftp://$UCLA_CLUSTER_HOSTNAME" mount-point="$UCLA_CLUSTER_WORK_DIR"/>
					<internal-mount-point mount-point="$UCLA_CLUSTER_WORK_DIR"/>
				</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="globus" key="maxwalltime">1430</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$UCLA_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$UCLA_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="env" key="BOWTIE_INDEXES">$UCLA_CLUSTER_HOME/bin/bowtieIndexes</profile>
		<profile namespace="env" key="HOME">$UCLA_CLUSTER_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$UCLA_CLUSTER_HOME/$VCF_PERL5LIB/</profile>
		<profile namespace="env" key="LD_LIBRARY_PATH" >$UCLA_CLUSTER_HOME/lib:/u/local/apps/python/2.6.5/lib:/u/local/intel/11.1/openmpi/1.4.2/lib:/u/local/compilers/intel/11.1/073/mkl/lib/em64t:/u/local/compilers/intel/11.1/073/lib/intel64</profile>
		
		<profile namespace="env" key="OMPI_MCA_mpi_leave_pinned">1</profile>
		<profile namespace="env" key="OMPI_MCA_mpi_warn_on_fork">0</profile>
		<profile namespace="env" key="PYTHON_DIR">/u/local/apps/python/2.6.5</profile>
		<profile namespace="env" key="PYTHON_INC">/u/local/apps/python/2.6.5/include/python2.6</profile>
		<profile namespace="env" key="PYTHON_LIB">/u/local/apps/python/2.6.5/lib</profile>
		<profile namespace="env" key="PATH" >$UCLA_CLUSTER_HOME/bin:/u/local/apps/lynx/2.8.7/bin:/u/local/apps/python/2.6.5/bin:/u/local/apps/python/2.6.5/bin/:/u/systems/SGE6.2u5/bin/lx26-amd64:/u/local/compilers/intel/11.1/073/bin/intel64/:/u/local/intel/11.1/openmpi/1.4.2/bin:/u/local/bin:/u/local/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin</profile>
		<profile namespace="env" key="PYTHONPATH">$UCLA_CLUSTER_HOME/lib/python:/u/local/apps/python/2.6.5/lib/python2.6/site-packages:/u/local/python/2.6/lib/python2.6/site-packages:/u/local/python/2.6/lib64/python2.6-other:$UCLA_CLUSTER_PEGASUS_PYTHON_LIB_DIR</profile>
	</site>
	<site  handle="uschpc" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="$USC_CLUSTER_HOSTNAME/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="$USC_CLUSTER_HOSTNAME/jobmanager-$USC_CLUSTER_SCHEDULER" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
				<shared>
					<file-server protocol="gsiftp" url="gsiftp://$USC_CLUSTER_HOSTNAME/" mount-point="$USC_CLUSTER_HOME/pg_work"/>
					<internal-mount-point mount-point="$USC_CLUSTER_HOME/pg_work" />
				</shared>
			</scratch>
			<storage />
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="env" key="PEGASUS_HOME" >$USC_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$USC_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="globus" key="queue" >cmb</profile>
		<profile namespace="globus" key="maxwalltime">4800</profile>
		<profile namespace="env" key="BOWTIE_INDEXES">$USC_CLUSTER_HOME/bin/bowtieIndexes</profile>
		<profile namespace="env" key="HOME">$USC_CLUSTER_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$USC_CLUSTER_HOME/$VCF_PERL5LIB/</profile>
		<profile namespace="env" key="PATH" >$USC_CLUSTER_HOME/bin:/usr/usc/python/default/bin/:/usr/usc/root/5.27.02/bin:/usr/usc/matlab/2009a/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/usr/usc/jdk/default/bin/</profile>
		<profile namespace="env" key="PYTHONPATH">$USC_CLUSTER_HOME/lib/python:/usr/usc/python/default/lib/python2.6/site-packages/:$USC_CLUSTER_PEGASUS_PYTHON_LIB_DIR</profile>
	</site>
</sitecatalog>
EOF
# plan and submit the  workflow

export CLASSPATH=.:$PEGASUS_HOME/lib/pegasus.jar:$CLASSPATH
echo Java CLASSPATH is $CLASSPATH

#2013.03.30 "--force " was once added due to a bug. it'll stop file reuse.
commandLine="pegasus-plan -Dpegasus.file.cleanup.clusters.size=$cleanupClustersSize --conf pegasusrc --sites $computingSiteName --dax $dagFile --dir work --relative-dir $submitFolderName --output-site $storageSiteName --cluster horizontal $submitOption"

echo commandLine is $commandLine

$commandLine

exitCode=$?
#2013.04.24
if test $exitCode = "0"; then
	echo work/$submitFolderName >> $runningWorkflowLogFname
else
	echo work/$submitFolderName >>$failedWorkflowLogFname
fi

# add the option below for debugging
#	-vvvvv \
#	--nocleanup \

# 3.0	-D pegasus.user.properties=pegasusrc \
