#!/bin/bash

set -e

TOPDIR=`pwd`

storageSiteNameDefault="local"
submitOptionDefault="--submit"
scratchTypeDefault="1"


# figure out where Pegasus is installed
export PEGASUS_HOME=`which pegasus-plan | sed 's/\/bin\/*pegasus-plan//'`
if [ "x$PEGASUS_HOME" = "x" ]; then
	echo "Unable to determine location of your Pegasus install"
	echo "Please make sure pegasus-plan is in your path"
	exit 1
fi 
echo $PEGASUS_HOME

if [ "x$GLOBUS_LOCATION" = "x" ]; then
	echo "Please set GLOBUS_LOCATION to the location of your Pegasus install"
	exit 1
fi 
 
echo GLOBUS_LOCATION: $GLOBUS_LOCATION

# 2011-8-28 same as the submitted user's home directory
# it's a must to export HOME in condor environment because HOME is not set by default.
CONDOR_HOME_DIR=$HOME

VCF_PERL5LIB=script/vcftools/perl

UCLA_CLUSTER_HOSTNAME="grid4.hoffman2.idre.ucla.edu"
UCLA_CLUSTER_SCHEDULER="sge"
UCLA_CLUSTER_HOME="/u/home/eeskin/polyacti"
UCLA_CLUSTER_PEGASUS_HOME="$UCLA_CLUSTER_HOME/bin/pegasus"
UCLA_CLUSTER_GLOBUS_LOCATION="/home/globus/gt5.0.4"
UCLA_CLUSTER_WORK_DIR=$UCLA_CLUSTER_HOME"/pg_work"

UCLA_CLUSTER_SCRATCH_Type1="/u/scratch2/p/polyacti/pegasus"
UCLA_CLUSTER_SCRATCH_Type2="$UCLA_CLUSTER_HOME/NetworkData/scratch/"
# 2013.3.6 on 2013.3.4, hoffman2 upgraded to the new /u/scratch2 system. this will phase out eventually
#UCLA_CLUSTER_SCRATCH_Type1_old="/u/scratch/p/polyacti/pegasus"

#UCLA_CLUSTER_LOCAL_SCRATCH="/work/polyacti/pegasus"
#if you choose hcondor as storage site, final output is staged out to local scratch!! make sure it's in shared storage.
UCLA_CLUSTER_LOCAL_SCRATCH=$UCLA_CLUSTER_SCRATCH

UCLA_CLUSTER_PYTHON_DIR="/u/local/python/2.6/"
UCLA_CLUSTER_R_DIR="/u/local/apps/R/R-2.12.2-gnu/"
UCLA_CLUSTER_HDF5_DIR="/u/local/intel/11.1/libs/hdf5/1.8.8-shared/"	#2013.1.10 not necessary

freeSpace="50000G"

USC_CLUSTER_HOSTNAME="hpc-login2.usc.edu"
USC_CLUSTER_SCHEDULER="pbs"
USC_CLUSTER_HOME="/home/cmb-03/mn/yuhuang"
USC_CLUSTER_WORK_DIR=$USC_CLUSTER_HOME"/pg_work"
USC_CLUSTER_PEGASUS_HOME=$USC_CLUSTER_HOME"/bin/pegasus"
USC_CLUSTER_GLOBUS_LOCATION="/usr/usc/globus/default/"
#USC_CLUSTER_GLOBUS_LOCATION="/usr/local/globus/default"


if test $# -lt 2 ; then
	echo "Please specify local or condorpool as argument. Examples:"
	echo "  $0 dagFile computingSiteName [keepIntermediateFiles] [scratchType] [submitOption] [storageSiteName] [finalOutputDir] [submitFolderName]"
	echo ""
	echo "Note:"
	echo "	#. computingSiteName is the site on which the computing jobs run. could be local, condorpool, hcondor, hoffman2, uschpc. The dagFile should match this."
	echo ""
	echo "	#. keepIntermediateFiles (setting it to 1 means no cleanup, otherwise or default is cleanup.). It changes the default submit option ($submitOptionDefault) to --submit --nocleanup."
	echo "	#. scratchType is the type of scratch file system to use."
	echo "		Only valid for hcondor site. "
	echo "			1: scratch FS is  $UCLA_CLUSTER_SCRATCH_Type1."
	echo "			2: scratch FS is  $UCLA_CLUSTER_SCRATCH_Type2."
	echo
	echo "	#. submitOption are options passed to pegasus-plan. Default is $submitOptionDefault. "
	echo "		'--submit' means pegasus will plan & submit the workflow."
	echo "		'--submit --nocleanup' means pegasus will not add cleanup jobs = all intermediate files will be kept."
	echo "		If you set it to empty string, '', only planning will be done but no submission."
	echo "		This option overwrites the keepIntermediateFiles option, which modifies the default submitOption."
	echo "	#. storageSiteName is the site to which final output is staged to. default is $storageSiteNameDefault."
	echo "	#. finalOutputDir is the directory which would contain the files requested to be transferred out by the end of the workflow. If it doesn't exist, pegasus would create one. Default is dagFile (without first folder if there is one) + year+date+time"
	echo "	#. submitFolderName is the submit folder which contains all job description files, job stdout/stderr output, logs, etc.It is optional. If not given, value of finalOutputDir is taken as submitFolderName."
	echo "	#. finalOutputDir and submitFolderName could be same. But they should be different from previous workflows."
	echo ""
	echo "Examples:"
	echo "	#plan & submit and do not keep intermediate files"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml condorpool"
	echo "	$0 dags/TrioInconsistency/TrioInconsistency15DistantVRC.xml condorpool 0"
	echo
	echo "	#plan & submit, keep intermediate files, run on scratch type 2 ($UCLA_CLUSTER_SCRATCH_Type2) on hcondor"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml hcondor 1 2"
	echo
	echo "	#only planning, no running (keepIntermediateFiles & scratchType does not matter) by assigning empty spaces to submitOption"
	echo "	$0 dags/TrioInconsistency15DistantVRC.xml condorpool 0 1 \"  \" "
	echo
	echo "	#run the workflow while keeping intermediate files. good for testing in which you often need to modify .dag.rescue log to backtrack finished jobs"
	echo "	$0 dags/TrioInconsistency/TrioInconsistency15DistantVRC.xml hcondor 1 1 \"--submit\" local TrioInconsistency/TrioInconsistency15DistantVRC_20110929T1726 TrioInconsistency/TrioInconsistency15DistantVRC_20110929T1726 "
	exit 1
fi

dagFile=$1
computingSiteName=$2
keepIntermediateFiles=$3
scratchType=$4
submitOption=$5
storageSiteName=$6
finalOutputDir=$7
submitFolderName=$8

#2013.2.12 no cleanup only when keepIntermediateFiles = 1
if test "$keepIntermediateFiles" = "1"; then
	submitOptionDefault="--submit --nocleanup"
fi

#2013.2.12 
if test "$scratchType" = "2"; then
	UCLA_CLUSTER_SCRATCH=$UCLA_CLUSTER_SCRATCH_Type2
else
	UCLA_CLUSTER_SCRATCH=$UCLA_CLUSTER_SCRATCH_Type1
fi

echo "Default submitOption is changed to $submitOptionDefault."

if test -z "$submitOption"
then
	submitOption=$submitOptionDefault
fi


if [ -z $storageSiteName ]; then
	storageSiteName=$storageSiteNameDefault
fi


if [ -z $finalOutputDir ]; then
	t=`python -c "import time; print time.asctime().split()[3].replace(':', '')"`
	month=`python -c "import time; print time.asctime().split()[1]"`
	day=`python -c "import time; print time.asctime().split()[2]"`
	year=`python -c "import time; print time.asctime().split()[-1]"`
	finalOutputDir=`python -c "import sys, os; pathLs= os.path.splitext(sys.argv[1])[0].split('/'); n=len(pathLs); print '/'.join(pathLs[-(n-1):])" $dagFile`.$year.$month.$day\T$t;
	echo Final output will be in $finalOutputDir
fi

if test -z "$submitFolderName"
then
	submitFolderName=$finalOutputDir
fi



echo "Submitting to $computingSiteName for computing."
echo "UCLA_CLUSTER_SCRATCH is $UCLA_CLUSTER_SCRATCH."
echo "storageSiteName is $storageSiteName."
echo "Final workflow submit option is $submitOption."



# 2012.7.31 the two lines below are added to any condor cluster that do not use shared file system or filesystem that is not good at handling numerous small files in one folder
#		<profile namespace="condor" key="should_transfer_files">YES</profile>
#		<profile namespace="condor" key="when_to_transfer_output">ON_EXIT_OR_EVICT</profile>

# create the site catalog
cat >sites.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0">
	<site  handle="local" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<head-fs>
			<scratch>
				<shared>
				<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
				<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</scratch>
			<storage>
				<shared>
				<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
				<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
				</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="env" key="PEGASUS_HOME" >$PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$GLOBUS_LOCATION</profile>
		<profile namespace="env" key="HOME">$HOME</profile>
	</site>
	<site  handle="condorpool" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
			<shared>
				<file-server protocol="file" url="file://" mount-point="$TOPDIR/scratch"/>
				<internal-mount-point mount-point="$TOPDIR/scratch" free-size="$freeSpace" total-size="$freeSpace"/>
			</shared>
			</scratch>
			<storage>
			<shared>
				<file-server protocol="file" url="file://" mount-point="$TOPDIR/$finalOutputDir"/>
				<internal-mount-point mount-point="$TOPDIR/$finalOutputDir" free-size="$freeSpace" total-size="$freeSpace"/>
			</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="pegasus" key="style" >condor</profile>
		<profile namespace="condor" key="universe" >vanilla</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$PEGASUS_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$CONDOR_HOME_DIR/$VCF_PERL5LIB</profile>
		<profile namespace="env" key="HOME" >$CONDOR_HOME_DIR</profile>
		<profile namespace="env" key="PATH" >$CONDOR_HOME_DIR/bin:$PATH</profile>
		<profile namespace="env" key="PYTHONPATH">$PYTHONPATH:$PEGASUS_HOME/lib/pegasus/python</profile>
	</site>
	<site  handle="hcondor" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
			<local>
				<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH" free-size="$freeSpace" total-size="$freeSpace"/>
			</local>
			<shared>
				<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_SCRATCH"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_SCRATCH" free-size="$freeSpace" total-size="$freeSpace"/>
			</shared>
			</scratch>
			<storage>
			<local>
				<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_LOCAL_SCRATCH" free-size="$freeSpace" total-size="$freeSpace"/>
			</local>
			<shared>
				<file-server protocol="file" url="file://" mount-point="$UCLA_CLUSTER_WORK_DIR"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_WORK_DIR" free-size="$freeSpace" total-size="$freeSpace"/>
			</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="pegasus" key="style" >condor</profile>
		<profile namespace="condor" key="universe" >vanilla</profile>
		<profile namespace="condor" key="should_transfer_files">YES</profile>
		<profile namespace="condor" key="when_to_transfer_output">ON_EXIT_OR_EVICT</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$UCLA_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$UCLA_CLUSTER_HOME/$VCF_PERL5LIB</profile>
		<profile namespace="env" key="HOME" >$UCLA_CLUSTER_HOME</profile>
		<profile namespace="env" key="PATH" >$UCLA_CLUSTER_HOME/bin:$PATH:$UCLA_CLUSTER_HDF5_DIR/bin</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$UCLA_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="env" key="LD_LIBRARY_PATH" >$UCLA_CLUSTER_HOME/lib:$UCLA_CLUSTER_R_DIR/lib64/R/lib:$UCLA_CLUSTER_PYTHON_DIR/lib:/u/local/intel/11.1/openmpi/1.4.2/lib:/u/local/compilers/intel/11.1/073/mkl/lib/em64t:/u/local/compilers/intel/11.1/073/lib/intel64:$UCLA_CLUSTER_HDF5_DIR/lib</profile>
		
		<profile namespace="env" key="OMPI_MCA_mpi_leave_pinned">1</profile>
		<profile namespace="env" key="OMPI_MCA_mpi_warn_on_fork">0</profile>
		<profile namespace="env" key="R_BIN">$UCLA_CLUSTER_R_DIR/bin</profile>
		<profile namespace="env" key="R_DIR">$UCLA_CLUSTER_R_DIR</profile>
		<profile namespace="env" key="R_HOME">$UCLA_CLUSTER_R_DIR/lib64/R</profile>
		<profile namespace="env" key="PYTHON_DIR">$UCLA_CLUSTER_PYTHON_DIR</profile>
		<profile namespace="env" key="PYTHON_INC">$UCLA_CLUSTER_PYTHON_DIR/include/python2.6</profile>
		<profile namespace="env" key="PYTHON_LIB">$UCLA_CLUSTER_PYTHON_DIR/lib</profile>
		<profile namespace="env" key="PYTHONPATH">$UCLA_CLUSTER_HOME/lib/python:$UCLA_CLUSTER_PYTHON_DIR/lib64/python2.6/site-packages:$UCLA_CLUSTER_PYTHON_DIR/lib/python2.6/site-packages:/u/home/eeskin/polyacti/bin/pegasus/lib/pegasus/python:$UCLA_CLUSTER_PYTHON_DIR/lib64/python2.6-other</profile>
	</site>
	<site  handle="hoffman2" arch="x86_64" os="LINUX">
		<grid  type="gt5" contact="$UCLA_CLUSTER_HOSTNAME/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt5" contact="$UCLA_CLUSTER_HOSTNAME/jobmanager-$UCLA_CLUSTER_SCHEDULER" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
			<shared>
				<file-server protocol="gsiftp" url="gsiftp://$UCLA_CLUSTER_HOSTNAME/" mount-point="$UCLA_CLUSTER_WORK_DIR"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_WORK_DIR" />
			</shared>
			</scratch>
			<storage>
			<shared>
				<file-server protocol="gsiftp" url="gsiftp://$UCLA_CLUSTER_HOSTNAME" mount-point="$UCLA_CLUSTER_WORK_DIR"/>
				<internal-mount-point mount-point="$UCLA_CLUSTER_WORK_DIR"/>
			</shared>
			</storage>
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="globus" key="maxwalltime">1430</profile>
		<profile namespace="env" key="PEGASUS_HOME" >$UCLA_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$UCLA_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="env" key="BOWTIE_INDEXES">$UCLA_CLUSTER_HOME/bin/bowtieIndexes</profile>
		<profile namespace="env" key="HOME">$UCLA_CLUSTER_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$UCLA_CLUSTER_HOME/$VCF_PERL5LIB/</profile>
		<profile namespace="env" key="LD_LIBRARY_PATH" >$UCLA_CLUSTER_HOME/lib:/u/local/apps/python/2.6.5/lib:/u/local/intel/11.1/openmpi/1.4.2/lib:/u/local/compilers/intel/11.1/073/mkl/lib/em64t:/u/local/compilers/intel/11.1/073/lib/intel64</profile>
		
		<profile namespace="env" key="OMPI_MCA_mpi_leave_pinned">1</profile>
		<profile namespace="env" key="OMPI_MCA_mpi_warn_on_fork">0</profile>
		<profile namespace="env" key="PYTHON_DIR">/u/local/apps/python/2.6.5</profile>
		<profile namespace="env" key="PYTHON_INC">/u/local/apps/python/2.6.5/include/python2.6</profile>
		<profile namespace="env" key="PYTHON_LIB">/u/local/apps/python/2.6.5/lib</profile>
		<profile namespace="env" key="PATH" >$UCLA_CLUSTER_HOME/bin:/u/local/apps/lynx/2.8.7/bin:/u/local/apps/python/2.6.5/bin:/u/local/apps/python/2.6.5/bin/:/u/systems/SGE6.2u5/bin/lx26-amd64:/u/local/compilers/intel/11.1/073/bin/intel64/:/u/local/intel/11.1/openmpi/1.4.2/bin:/u/local/bin:/u/local/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin</profile>
		<profile namespace="env" key="PYTHONPATH">$UCLA_CLUSTER_HOME/lib/python:/u/local/apps/python/2.6.5/lib/python2.6/site-packages:/u/local/python/2.6/lib/python2.6/site-packages:/u/local/python/2.6/lib64/python2.6-other</profile>
	</site>
	<site  handle="uschpc" arch="x86_64" os="LINUX">
		<grid  type="gt2" contact="$USC_CLUSTER_HOSTNAME/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/>
		<grid  type="gt2" contact="$USC_CLUSTER_HOSTNAME/jobmanager-$USC_CLUSTER_SCHEDULER" scheduler="unknown" jobtype="compute"/>
		<head-fs>
			<scratch>
			<shared>
				<file-server protocol="gsiftp" url="gsiftp://$USC_CLUSTER_HOSTNAME/" mount-point="$USC_CLUSTER_HOME/pg_work"/>
				<internal-mount-point mount-point="$USC_CLUSTER_HOME/pg_work" />
			</shared>
			</scratch>
			<storage />
		</head-fs>
		<replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" />
		<profile namespace="env" key="PEGASUS_HOME" >$USC_CLUSTER_PEGASUS_HOME</profile>
		<profile namespace="env" key="GLOBUS_LOCATION" >$USC_CLUSTER_GLOBUS_LOCATION</profile>
		<profile namespace="globus" key="queue" >cmb</profile>
		<profile namespace="globus" key="maxwalltime">4800</profile>
		<profile namespace="env" key="BOWTIE_INDEXES">$USC_CLUSTER_HOME/bin/bowtieIndexes</profile>
		<profile namespace="env" key="HOME">$USC_CLUSTER_HOME</profile>
		<profile namespace="env" key="PERL5LIB">$USC_CLUSTER_HOME/$VCF_PERL5LIB/</profile>
		<profile namespace="env" key="PATH" >$USC_CLUSTER_HOME/bin:/usr/usc/python/default/bin/:/usr/usc/root/5.27.02/bin:/usr/usc/matlab/2009a/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/usr/usc/jdk/default/bin/</profile>
		<profile namespace="env" key="PYTHONPATH">$USC_CLUSTER_HOME/lib/python:/usr/usc/python/default/lib/python2.6/site-packages/</profile>
	</site>
</sitecatalog>
EOF
# plan and submit the  workflow

export CLASSPATH=.:$PEGASUS_HOME/lib/pegasus.jar:$CLASSPATH
echo $CLASSPATH

pegasus-plan \
	--conf pegasusrc \
	--sites $computingSiteName \
	--dir work \
	--relative-dir $submitFolderName \
	--dax $dagFile \
	--output $storageSiteName \
	--cluster horizontal $submitOption
# add the option below for debugging
#	-vvvvv \
#	--nocleanup \

# 3.0	-D pegasus.user.properties=pegasusrc \
